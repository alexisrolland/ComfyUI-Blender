{
  "1": {
    "inputs": {
      "model_name": "alibaba\\svdq-int4_r128-qwen-image-edit-2509-lightningv2.0-4steps.safetensors",
      "cpu_offload": "auto",
      "num_blocks_on_gpu": 60,
      "use_pin_memory": "disable"
    },
    "class_type": "NunchakuQwenImageDiTLoader",
    "_meta": {
      "title": "Nunchaku Qwen-Image DiT Loader"
    }
  },
  "2": {
    "inputs": {
      "shift": 3,
      "model": [
        "1",
        0
      ]
    },
    "class_type": "ModelSamplingAuraFlow",
    "_meta": {
      "title": "ModelSamplingAuraFlow"
    }
  },
  "3": {
    "inputs": {
      "strength": 1,
      "model": [
        "2",
        0
      ]
    },
    "class_type": "CFGNorm",
    "_meta": {
      "title": "CFGNorm"
    }
  },
  "4": {
    "inputs": {
      "clip_name": "alibaba\\qwen_2.5_vl_7b_fp8_scaled.safetensors",
      "type": "qwen_image",
      "device": "default"
    },
    "class_type": "CLIPLoader",
    "_meta": {
      "title": "Load CLIP"
    }
  },
  "5": {
    "inputs": {
      "prompt": [
        "13",
        0
      ],
      "clip": [
        "4",
        0
      ],
      "vae": [
        "6",
        0
      ]
    },
    "class_type": "TextEncodeQwenImageEditPlus",
    "_meta": {
      "title": "TextEncodeQwenImageEditPlus"
    }
  },
  "6": {
    "inputs": {
      "vae_name": "alibaba\\qwen_image_vae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "7": {
    "inputs": {
      "seed": [
        "14",
        0
      ],
      "steps": 4,
      "cfg": 1,
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "3",
        0
      ],
      "positive": [
        "8",
        0
      ],
      "negative": [
        "9",
        0
      ],
      "latent_image": [
        "12",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "8": {
    "inputs": {
      "conditioning": [
        "5",
        0
      ],
      "latent": [
        "11",
        0
      ]
    },
    "class_type": "ReferenceLatent",
    "_meta": {
      "title": "ReferenceLatent"
    }
  },
  "9": {
    "inputs": {
      "conditioning": [
        "5",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "ConditioningZeroOut"
    }
  },
  "10": {
    "inputs": {
      "image": "example.png",
      "order": 4
    },
    "class_type": "BlenderInputLoadImage",
    "_meta": {
      "title": "Image"
    }
  },
  "11": {
    "inputs": {
      "pixels": [
        "10",
        0
      ],
      "vae": [
        "6",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "12": {
    "inputs": {
      "width": [
        "15",
        0
      ],
      "height": [
        "16",
        0
      ],
      "batch_size": 1
    },
    "class_type": "EmptySD3LatentImage",
    "_meta": {
      "title": "EmptySD3LatentImage"
    }
  },
  "13": {
    "inputs": {
      "value": "Photograph of an ancient treasure chest on a black background. The chest is made of rich, vintage antique wood sculpted with floral patterns, with brass and iron fittings. The brass lock and hinges are tarnished with age, adding to its mysterious and timeless appeal. The lighting is dramatic, with soft shadows accentuating the texture of the wood and metal, creating a sense of depth and intrigue. Ultra HD, 4K, cinematic composition.",
      "order": 2,
      "default": "",
      "format_path": false
    },
    "class_type": "BlenderInputStringMultiline",
    "_meta": {
      "title": "Prompt"
    }
  },
  "14": {
    "inputs": {
      "value": 0,
      "order": 1,
      "default": 0,
      "min": 0,
      "max": 2147483647,
      "step": 1
    },
    "class_type": "BlenderInputSeed",
    "_meta": {
      "title": "Seed"
    }
  },
  "15": {
    "inputs": {
      "value": 1328,
      "order": 1,
      "default": 1328,
      "min": 928,
      "max": 1664,
      "step": 16,
      "camera_width": true,
      "camera_height": false,
      "group": [
        "19",
        0
      ]
    },
    "class_type": "BlenderInputInt",
    "_meta": {
      "title": "Width"
    }
  },
  "16": {
    "inputs": {
      "value": 1328,
      "order": 2,
      "default": 1328,
      "min": 928,
      "max": 1664,
      "step": 16,
      "camera_width": false,
      "camera_height": true,
      "group": [
        "19",
        0
      ]
    },
    "class_type": "BlenderInputInt",
    "_meta": {
      "title": "Height"
    }
  },
  "17": {
    "inputs": {
      "samples": [
        "7",
        0
      ],
      "vae": [
        "6",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "18": {
    "inputs": {
      "filename_prefix": "blender",
      "images": [
        "17",
        0
      ]
    },
    "class_type": "BlenderOutputSaveImage",
    "_meta": {
      "title": "Save Image"
    }
  },
  "19": {
    "inputs": {
      "order": 3,
      "show_title": true,
      "compact": true
    },
    "class_type": "BlenderInputGroup",
    "_meta": {
      "title": "Resolution"
    }
  }
}